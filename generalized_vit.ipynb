{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkX1FdBOJvqkCd1UTIqKna",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/acv_final_project/blob/main/generalized_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2eajBf5IVew",
        "outputId": "6c04f45f-96a7-4768-be57-ccbafb035291"
      },
      "source": [
        "!pip install vit-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/31/40/56919a1be6b596f30a692d4855f2d7bde8945e95cd4eb1c6588c109d4581/vit_pytorch-0.6.7-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from vit-pytorch) (1.7.0+cu101)\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (0.8)\n",
            "Installing collected packages: einops, vit-pytorch\n",
            "Successfully installed einops-0.3.0 vit-pytorch-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9AtrkoDIYUV"
      },
      "source": [
        "import torch\r\n",
        "from vit_pytorch import ViT"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rowCZ5jCI1cs"
      },
      "source": [
        "from einops import rearrange, repeat"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8PpgIUJRlR"
      },
      "source": [
        "img = torch.randn(1, 3, 224, 224)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvSr-i0zMhK0"
      },
      "source": [
        "from torch.nn import Conv2d"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daEJ2unNMz9u"
      },
      "source": [
        "patch_size = 32"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OvaoDYRMmrJ"
      },
      "source": [
        "patch_embeddings = Conv2d(in_channels=3,\r\n",
        "                                       out_channels=128,\r\n",
        "                                       kernel_size=patch_size,\r\n",
        "                                       stride=patch_size//2)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCcC2JwEM2xG",
        "outputId": "0c3d85f4-b2d2-4637-b239-16bdc97ea9b6"
      },
      "source": [
        "patch_embeddings(img).shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 13, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvOod4cJTPbI",
        "outputId": "f56edada-661b-4c68-d337-3f1bdbd748f8"
      },
      "source": [
        " t = torch.range(1, 256).reshape((16, 16))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv9f6NyeTW5P"
      },
      "source": [
        "input = torch.randn(1, 3, 224, 224)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IwUzSaZTdo5"
      },
      "source": [
        "unfold = torch.nn.Unfold(kernel_size=(32, 32), stride=(16,16))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OwhNTFfU4dN"
      },
      "source": [
        "output = unfold(input)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQTHxEhcVMFP",
        "outputId": "9c77749a-407b-4b70-eccf-e188c397b734"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3072, 169])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Hubfp9Wfnh",
        "outputId": "48a54fe5-4da5-4460-fd14-9b05e38c5eee"
      },
      "source": [
        "output.permute(0,2,1).shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 169, 3072])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNp7JvOpYq17"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from einops import rearrange, repeat\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "MIN_NUM_PATCHES = 16\r\n",
        "\r\n",
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.fn = fn\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(x, **kwargs) + x\r\n",
        "\r\n",
        "class PreNorm(nn.Module):\r\n",
        "    def __init__(self, dim, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.LayerNorm(dim)\r\n",
        "        self.fn = fn\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(self.norm(x), **kwargs)\r\n",
        "\r\n",
        "class FeedForward(nn.Module):\r\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\r\n",
        "        super().__init__()\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Linear(dim, hidden_dim),\r\n",
        "            nn.GELU(),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(hidden_dim, dim),\r\n",
        "            nn.Dropout(dropout)\r\n",
        "        )\r\n",
        "    def forward(self, x):\r\n",
        "        return self.net(x)\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\r\n",
        "        super().__init__()\r\n",
        "        inner_dim = dim_head *  heads\r\n",
        "        self.heads = heads\r\n",
        "        self.scale = dim_head ** -0.5\r\n",
        "\r\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\r\n",
        "        self.to_out = nn.Sequential(\r\n",
        "            nn.Linear(inner_dim, dim),\r\n",
        "            nn.Dropout(dropout)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x, mask = None):\r\n",
        "        b, n, _, h = *x.shape, self.heads\r\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\r\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\r\n",
        "\r\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\r\n",
        "        mask_value = -torch.finfo(dots.dtype).max\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\r\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\r\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\r\n",
        "            dots.masked_fill_(~mask, mask_value)\r\n",
        "            del mask\r\n",
        "\r\n",
        "        attn = dots.softmax(dim=-1)\r\n",
        "\r\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\r\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\r\n",
        "        out =  self.to_out(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.ModuleList([])\r\n",
        "        for _ in range(depth):\r\n",
        "            self.layers.append(nn.ModuleList([\r\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\r\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\r\n",
        "            ]))\r\n",
        "    def forward(self, x, mask = None):\r\n",
        "        for attn, ff in self.layers:\r\n",
        "            x = attn(x, mask = mask)\r\n",
        "            x = ff(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "class ViT(nn.Module):\r\n",
        "    def __init__(self, *, image_size, patch_size, stride, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\r\n",
        "        super().__init__()\r\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\r\n",
        "        num_patches = (((image_size - patch_size)//stride)+1)**2    ###(image_size // patch_size) ** 2\r\n",
        "        patch_dim = channels * patch_size ** 2\r\n",
        "        assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\r\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\r\n",
        "\r\n",
        "        self.patch_size = patch_size\r\n",
        "        self.stride_size = stride\r\n",
        "        self.unfolder = torch.nn.Unfold(kernel_size=(self.patch_size, self.patch_size), stride=(self.stride_size,self.stride_size)) \r\n",
        "\r\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\r\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\r\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\r\n",
        "        self.dropout = nn.Dropout(emb_dropout)\r\n",
        "\r\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\r\n",
        "\r\n",
        "        self.pool = pool\r\n",
        "        self.to_latent = nn.Identity()\r\n",
        "\r\n",
        "        self.mlp_head = nn.Sequential(\r\n",
        "            nn.LayerNorm(dim),\r\n",
        "            nn.Linear(dim, num_classes)\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, img, mask = None):\r\n",
        "        p = self.patch_size\r\n",
        "\r\n",
        "        # x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\r\n",
        "        x = self.unfolder(img).permute(0,2,1)\r\n",
        "        x = self.patch_to_embedding(x)\r\n",
        "        b, n, _ = x.shape\r\n",
        "\r\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\r\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\r\n",
        "        x += self.pos_embedding[:, :(n + 1)]\r\n",
        "        x = self.dropout(x)\r\n",
        "\r\n",
        "        x = self.transformer(x, mask)\r\n",
        "\r\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\r\n",
        "\r\n",
        "        x = self.to_latent(x)\r\n",
        "        return self.mlp_head(x)\r\n"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzkOWzrLZhf_"
      },
      "source": [
        "v = ViT(\r\n",
        "    image_size = 224,\r\n",
        "    patch_size = 32,\r\n",
        "    stride = 16,\r\n",
        "    num_classes = 1000,\r\n",
        "    dim = 1024,\r\n",
        "    depth = 6,\r\n",
        "    heads = 16,\r\n",
        "    mlp_dim = 2048,\r\n",
        "    dropout = 0.1,\r\n",
        "    emb_dropout = 0.1\r\n",
        ")"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNt39-cXZl1z"
      },
      "source": [
        "img = torch.randn(1, 3, 224, 224)\r\n",
        "\r\n",
        "####num_patches = (((image_size - patch_size)//stride)+1)**2\r\n",
        "\r\n",
        "mask = torch.ones(1, 13, 13).bool() # optional mask, designating which patch to attend to\r\n",
        "\r\n",
        "preds = v(img, mask = mask) # (1, 1000)\r\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Sc5qMheXxy",
        "outputId": "38e93391-3296-4209-c949-705d956fd0f1"
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvIf2ASBemaH",
        "outputId": "83d7f385-5a52-455d-f0d4-1bbb1ce559c6"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "count_parameters(v)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54729704"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    }
  ]
}