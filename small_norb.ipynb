{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "small_norb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtSWQ5R2zY/r4RMgEncCxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/acv_final_project/blob/main/small_norb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfH2_TUqgOYg",
        "outputId": "8c0e3471-40d5-492b-9a70-d56ab7c540fe"
      },
      "source": [
        "!pip install timm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/ba02d533cec7329323c7d7a317ab49f673846ecef202d4cc40988b6b7786/timm-0.3.4-py3-none-any.whl (244kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 12.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 17.9MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 51kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 204kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 215kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 225kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 235kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.6/dist-packages (from timm) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm) (0.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (7.0.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd-6lUayXpm9"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torchvision\r\n",
        "import torch\r\n",
        "import timm\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlZVOW0nX0Rr"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4zlOPdkUSOG"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from PIL import Image\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "import struct\r\n",
        "import math\r\n",
        "if sys.version_info[0] == 2:\r\n",
        "    import cPickle as pickle\r\n",
        "else:\r\n",
        "    import pickle\r\n",
        "\r\n",
        "import torch.utils.data as data\r\n",
        "from torchvision.datasets.utils import download_url, check_integrity\r\n",
        "\r\n",
        "class smallNORB(data.Dataset):\r\n",
        "    \"\"\"`small NORB <https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/>`_ Dataset.\r\n",
        "    Args:\r\n",
        "        root (string): Root directory of dataset\r\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\r\n",
        "            creates from test set.\r\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\r\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\r\n",
        "        target_transform (callable, optional): A function/transform that takes in the\r\n",
        "            target and transforms it.\r\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\r\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\r\n",
        "            downloaded again.\r\n",
        "    \"\"\"\r\n",
        "    urls_train = [\r\n",
        "        [\r\n",
        "            \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat.gz\",\r\n",
        "            \"smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat.gz\",\r\n",
        "            \"66054832f9accfe74a0f4c36a75bc0a2\"\r\n",
        "        ],\r\n",
        "        [\r\n",
        "            \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat.gz\",\r\n",
        "            \"smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat.gz\",\r\n",
        "            \"23c8b86101fbf0904a000b43d3ed2fd9\"\r\n",
        "        ],\r\n",
        "        [\r\n",
        "            \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/smallnorb-5x46789x9x18x6x2x96x96-training-info.mat.gz\",\r\n",
        "            \"smallnorb-5x46789x9x18x6x2x96x96-training-info.mat.gz\",\r\n",
        "            \"51dee1210a742582ff607dfd94e332e3\"\r\n",
        "        ]\r\n",
        "    ]\r\n",
        "    urls_test = [\r\n",
        "        [\r\n",
        "            \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat.gz\",\r\n",
        "            \"smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat.gz\",\r\n",
        "            \"e4ad715691ed5a3a5f138751a4ceb071\"\r\n",
        "        ],\r\n",
        "        [\r\n",
        "            \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat.gz\",\r\n",
        "            \"smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat.gz\",\r\n",
        "            \"5aa791cd7e6016cf957ce9bdb93b8603\"\r\n",
        "        ],\r\n",
        "        [\r\n",
        "            \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat.gz\",\r\n",
        "            \"smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat.gz\",\r\n",
        "            \"a9454f3864d7fd4bb3ea7fc3eb84924e\"\r\n",
        "        ]\r\n",
        "    ]\r\n",
        "\r\n",
        "    train_data_file = [\"smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat\", \"8138a0902307b32dfa0025a36dfa45ec\"]\r\n",
        "    train_labels_file = [\"smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat\", \"fd5120d3f770ad57ebe620eb61a0b633\"]\r\n",
        "    train_info_file = [\"smallnorb-5x46789x9x18x6x2x96x96-training-info.mat\", \"19faee774120001fc7e17980d6960451\"]\r\n",
        "\r\n",
        "    test_data_file =  [\"smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat\", \"e9920b7f7b2869a8f1a12e945b2c166c\"]\r\n",
        "    test_labels_file = [\"smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat\", \"fd5120d3f770ad57ebe620eb61a0b633\"]\r\n",
        "    test_info_file = [\"smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat\", \"7c5b871cc69dcadec1bf6a18141f5edc\"]\r\n",
        "\r\n",
        "    def __init__(self, root, train=True,\r\n",
        "                 transform=None, target_transform=None,\r\n",
        "                 download=False):\r\n",
        "        self.root = os.path.expanduser(root)\r\n",
        "        self.transform = transform\r\n",
        "        self.target_transform = target_transform\r\n",
        "        self.train = train  # training set or test set\r\n",
        "\r\n",
        "        if download:\r\n",
        "            self.download()\r\n",
        "\r\n",
        "        if not self._check_integrity():\r\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\r\n",
        "                               ' You can use download=True to download it')\r\n",
        "\r\n",
        "        if self.train:\r\n",
        "            self.train_data = []\r\n",
        "            self.train_labels = []\r\n",
        "            self.train_info = []\r\n",
        "\r\n",
        "            with open(os.path.join(self.root, self.train_data_file[0]), mode='rb') as f:\r\n",
        "                self.train_data = self._parse_small_norb_data(f)\r\n",
        "            with open(os.path.join(self.root, self.train_labels_file[0]), mode='rb') as f:\r\n",
        "                self.train_labels = self._parse_small_norb_labels(f)\r\n",
        "            with open(os.path.join(self.root, self.train_info_file[0]), mode='rb') as f:\r\n",
        "                self.train_info = self._parse_small_norb_info(f)\r\n",
        "        else:\r\n",
        "            with open(os.path.join(self.root, self.test_data_file[0]), mode='rb') as f:\r\n",
        "                self.test_data = self._parse_small_norb_data(f)\r\n",
        "            with open(os.path.join(self.root, self.test_labels_file[0]), mode='rb') as f:\r\n",
        "                self.test_labels = self._parse_small_norb_labels(f)\r\n",
        "            with open(os.path.join(self.root, self.test_info_file[0]), mode='rb') as f:\r\n",
        "                self.test_info = self._parse_small_norb_info(f)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            index (int): Index\r\n",
        "        Returns:\r\n",
        "            tuple: (image, target) where target is index of the target class.\r\n",
        "        \"\"\"\r\n",
        "        dindex = math.floor(index/2)\r\n",
        "        if self.train:\r\n",
        "            img, target, info = self.train_data[index], self.train_labels[dindex], self.train_info[dindex]\r\n",
        "        else:\r\n",
        "            img, target, info = self.test_data[index], self.test_labels[dindex], self.test_info[dindex]\r\n",
        "\r\n",
        "        if self.transform is not None:\r\n",
        "            img = self.transform(img)\r\n",
        "\r\n",
        "        if self.target_transform is not None:\r\n",
        "            target = self.target_transform(target)\r\n",
        "\r\n",
        "        img = img.convert('RGB')\r\n",
        "        img = np.array(img.resize((224,224)))\r\n",
        "        img = img.transpose((2, 0, 1))\r\n",
        "        target = np.array([target])\r\n",
        "\r\n",
        "        return {'image': torch.from_numpy(img),\r\n",
        "                'target': torch.from_numpy(target)}\r\n",
        "\r\n",
        "\r\n",
        "        #return img, target\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        if self.train:\r\n",
        "            return len(self.train_data)\r\n",
        "        else:\r\n",
        "            return len(self.test_data)\r\n",
        "\r\n",
        "    def _check_integrity(self):\r\n",
        "        root = self.root\r\n",
        "        for fentry in ([\r\n",
        "            self.train_data_file, \r\n",
        "            self.train_labels_file,\r\n",
        "            self.train_info_file,\r\n",
        "            self.test_data_file, \r\n",
        "            self.test_labels_file,\r\n",
        "            self.test_info_file\r\n",
        "        ]):\r\n",
        "            filename, md5 = fentry[0], fentry[1]\r\n",
        "            fpath = os.path.join(root, filename)\r\n",
        "            if not check_integrity(fpath, md5):\r\n",
        "                return False\r\n",
        "        return True\r\n",
        "\r\n",
        "    def download(self):\r\n",
        "        import gzip\r\n",
        "        import shutil\r\n",
        "\r\n",
        "        if self._check_integrity():\r\n",
        "            print('Files already downloaded and verified')\r\n",
        "            return\r\n",
        "\r\n",
        "        root = self.root\r\n",
        "\r\n",
        "        for url in (self.urls_train + self.urls_test):\r\n",
        "            download_url(url[0], root, url[1], url[2])\r\n",
        "\r\n",
        "            with gzip.open(os.path.join(root, url[1]), 'rb') as f_in:\r\n",
        "                with open(os.path.join(root, url[1][:-3]), 'wb') as f_out:\r\n",
        "                    shutil.copyfileobj(f_in, f_out)\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\r\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\r\n",
        "        tmp = 'train' if self.train is True else 'test'\r\n",
        "        fmt_str += '    Split: {}\\n'.format(tmp)\r\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\r\n",
        "        tmp = '    Transforms (if any): '\r\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\r\n",
        "        tmp = '    Target Transforms (if any): '\r\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\r\n",
        "        return fmt_str\r\n",
        "\r\n",
        "    def _parse_small_norb_header(self, file):\r\n",
        "        magic = struct.unpack('<BBBB', file.read(4))\r\n",
        "        ndims = struct.unpack('<i', file.read(4))[0]\r\n",
        "        shape = []\r\n",
        "        for _ in range(ndims):\r\n",
        "            shape.append(struct.unpack('<i', file.read(4)))\r\n",
        "        return {'magic_number' : magic, 'shape' : shape}\r\n",
        "\r\n",
        "    def _parse_small_norb_data(self, file):\r\n",
        "        self._parse_small_norb_header(file)\r\n",
        "        data = []\r\n",
        "        buf  = file.read(9216)\r\n",
        "        while len(buf):\r\n",
        "            data.append(Image.frombuffer('L', (96,96), buf, 'raw', 'L', 0, 1))\r\n",
        "            buf = file.read(9216)\r\n",
        "        return data\r\n",
        "\r\n",
        "    def _parse_small_norb_labels(self, file):\r\n",
        "        self._parse_small_norb_header(file)\r\n",
        "        file.read(8)\r\n",
        "        data = []\r\n",
        "        buf  = file.read(4)\r\n",
        "        while len(buf):\r\n",
        "            data.append(struct.unpack('<i', buf)[0])\r\n",
        "            buf = file.read(4)\r\n",
        "        return data\r\n",
        "\r\n",
        "    def _parse_small_norb_info(self, file):\r\n",
        "        self._parse_small_norb_header(file)\r\n",
        "        file.read(4)\r\n",
        "        instance = []\r\n",
        "        elevation = []\r\n",
        "        azimuth = []\r\n",
        "        lighting = []\r\n",
        "        buf  = file.read(4)\r\n",
        "        while len(buf):\r\n",
        "            instance.append(struct.unpack('<i', buf)[0])\r\n",
        "            buf = file.read(4)\r\n",
        "            elevation.append(struct.unpack('<i', buf)[0])\r\n",
        "            buf = file.read(4)\r\n",
        "            azimuth.append(struct.unpack('<i', buf)[0])\r\n",
        "            buf = file.read(4)\r\n",
        "            lighting.append(struct.unpack('<i', buf)[0])\r\n",
        "            buf = file.read(4)\r\n",
        "        return np.array([instance, elevation, azimuth, lighting]).transpose()"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1-SOb2KUulx",
        "outputId": "b5847407-a2cf-4ed2-c63f-b8284768a806"
      },
      "source": [
        "toy_dataset_train = smallNORB('data/', train=True, download=True)\r\n",
        "toy_dataset_test = smallNORB('data/', train=False, download=True)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGcuVN4oPy-"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "train_dataloader = DataLoader(toy_dataset_train, batch_size=16, shuffle=True)\r\n",
        "test_dataloader = DataLoader(toy_dataset_test, batch_size=16, shuffle=True)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iczYTU07x56d"
      },
      "source": [
        "class CNNNet(torch.nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(CNNNet, self).__init__()\r\n",
        "    self.cnn = timm.create_model('resnet50', pretrained=True, num_classes=0)\r\n",
        "    self.fc = torch.nn.Linear(2048, 5)\r\n",
        "    self.cnn = self.cnn.eval()\r\n",
        "    for param in self.cnn.parameters():\r\n",
        "      param.requires_grad = False\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    # x = [batch, channel, 224, 224]\r\n",
        "    temp = self.cnn(x)\r\n",
        "    temp = self.fc(temp)\r\n",
        "    return temp\r\n",
        "\r\n",
        "class TransformerNet(torch.nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(TransformerNet, self).__init__()\r\n",
        "    self.transformer = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\r\n",
        "    self.fc = torch.nn.Linear(768, 5)\r\n",
        "    self.transformer = self.transformer.eval()\r\n",
        "    for param in self.transformer.parameters():\r\n",
        "      param.requires_grad = False\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    # x = [batch, channel, 224, 224]\r\n",
        "    temp = self.transformer(x)\r\n",
        "    temp = self.fc(temp)\r\n",
        "    return temp\r\n",
        "\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqHnwMp2ybR_",
        "outputId": "ef6bc73e-44d4-4e00-bee7-e48b2da63f11"
      },
      "source": [
        "x = torch.rand(4, 3, 224, 224)\r\n",
        "m1 = CNNNet()\r\n",
        "m2 = TransformerNet()\r\n",
        "\r\n",
        "count_parameters(m1), count_parameters(m2)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10245, 3845)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKt_goWS2AZA"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "optimizer = optim.Adam(params =m1.fc.parameters() , lr=1e-5)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "BSYd8NV7zjSn",
        "outputId": "04e6fd08-6148-4a7c-c1ed-d06ccd753ef0"
      },
      "source": [
        "MAX_EPOCH = 5\r\n",
        "\r\n",
        "for epoch in range(MAX_EPOCH):\r\n",
        "  running_loss = 0.0\r\n",
        "\r\n",
        "  for i, batch in enumerate(train_dataloader):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    batch_x = batch['image']\r\n",
        "    batch_y = batch['target']\r\n",
        "    y_pred = m1(batch_x.float())\r\n",
        "    loss = criterion(y_pred, batch_y.squeeze(1))\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    running_loss += loss.item()\r\n",
        "    if i % 200 == 199:    # print every 200 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-195-66b919bf0273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
          ]
        }
      ]
    }
  ]
}